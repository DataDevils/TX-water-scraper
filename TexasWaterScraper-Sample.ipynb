{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texas Water Scraper\n",
    "* Get a list of sites for the year to scrape\n",
    "* Loop through each size; generate and pull a report\n",
    "* Scrape the report details into a csv file\n",
    "---\n",
    "Requires:\n",
    "* The selenium package and the geckodriver.exe file (for Mozilla browsers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os, glob, time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings\n",
    "year = 2016\n",
    "theURL = 'http://www2.twdb.texas.gov/ReportServerExt/Pages/ReportViewer.aspx?%2fWU%2fSumFinal_WUG_Entity_Detail_{}&rs:Command=Render'.format(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape a list of utilities from the base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the page and convert it to 'soup'\n",
    "response = requests.get(theURL)\n",
    "soup = BeautifulSoup(response.content,'lxml')\n",
    "\n",
    "#Extract the control from the 'soup'\n",
    "tbl = soup.find(id=\"ReportViewerControl_ctl04_ctl03\")\n",
    "\n",
    "#Create a list of utilities and populate values from each option in the control\n",
    "utilities = []\n",
    "values = []\n",
    "for option in tbl.findAll('option')[1:]: #Skips the first item\n",
    "    #Remove unicode chars\n",
    "    utility = unicodedata.normalize(\"NFKD\",option.text)\n",
    "    utilities.append(utility)\n",
    "    values.append(option.attrs['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loop through each utility and extract its report\n",
    "This step requires the selenium package to open a remotely controlled browser, mimic selecting the utility and clicking the View Report button, then waiting for the report to complete and appear. Once that occurs the contents stored as a local HTML file for later scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->AUSTIN\n",
      "  fetching report\n",
      "Complete! Saved to AUSTIN_2016.html\n",
      "True\n",
      "   fetching 2nd page... Done\n"
     ]
    }
   ],
   "source": [
    "#Create the selenium browser object\n",
    "browser = webdriver.Firefox(executable_path='./geckodriver.exe')\n",
    "\n",
    "#Open the page in the selenium browser\n",
    "browser.get(theURL)\n",
    "\n",
    "utility = 'AUSTIN'\n",
    "value = '53'\n",
    "        \n",
    "#Set the output filename from the utility and year values\n",
    "outFN = \"{}_{}.html\".format(utility.replace(\" \",\"_\"),year)\n",
    "\n",
    "#Skip if already downloaded\n",
    "#if os.path.exists(outFN):\n",
    "    #print(\"Already processed.\")\n",
    "    #continue\n",
    "\n",
    "#Status\n",
    "print(\"->{}\".format(utility))\n",
    "\n",
    "try:\n",
    "    #Select the item and 'click' it\n",
    "    utilControl = Select(browser.find_element_by_id('ReportViewerControl_ctl04_ctl03_ddValue'))\n",
    "    utilControl.select_by_value(value)\n",
    "except: \n",
    "    print(\"Selector not found for {}\".format(utility))\n",
    "\n",
    "\n",
    "\n",
    "#Select the View Report button to retrieve the report\n",
    "browser.find_element_by_xpath('//*[@id=\"ReportViewerControl_ctl04_ctl00\"]').click()\n",
    "\n",
    "#Wait for the report to complete (i.e. when the utility's name appears on the active page)\n",
    "searchText = \"{}</div>\".format(utility)\n",
    "\n",
    "print(\"  fetching report\")\n",
    "\n",
    "while True:\n",
    "    #Get the page source\n",
    "    pageSource = browser.page_source\n",
    "\n",
    "    #Trigger the action when then \"Cancel\" control disappears\n",
    "    if not browser.find_element_by_id('ReportViewerControl_AsyncWait_Wait').is_displayed():\n",
    "        print(\"Complete! Saved to {}\".format(outFN))\n",
    "\n",
    "        #Write the page\n",
    "        with open(outFN,'w') as outfile:\n",
    "           outfile.write(pageSource)\n",
    "\n",
    "        #Check to see another page of data exists\n",
    "        soup = BeautifulSoup(pageSource,'lxml')\n",
    "        pageCtrl = soup.find(id='ReportViewerControl_ctl05_ctl00_TotalPages')\n",
    "        print(pageCtrl.contents[0] != '2 ?')\n",
    "        if pageCtrl.contents[0] != '1 ?': # There's a 2nd page\n",
    "            print(\"   fetching 2nd page\",end=\"... \")\n",
    "\n",
    "            #Click the next page button\n",
    "            ctrl=browser.find_element_by_xpath('//*[@id=\"ReportViewerControl_ctl05_ctl00_Next_ctl00_ctl00\"]').click()\n",
    "\n",
    "            #Wait until page refreshes (the '?' will disappear), then add it's contents\n",
    "            start_time = time.time()\n",
    "            while True:\n",
    "                pageSource = browser.page_source\n",
    "                elapsed_time = time.time() - start_time\n",
    "                ctrl = BeautifulSoup(pageSource,'lxml').find(id='ReportViewerControl_ctl05_ctl00_TotalPages')\n",
    "                #Continue to loop (wait) until the '?' disappears\n",
    "                if \"?\" not in ctrl.contents[0]: \n",
    "                    #Append contents to out file\n",
    "                    outFN2 = outFN[:-5]+\"_pg2.html\"\n",
    "                    with open(outFN2,'w') as outfile:\n",
    "                       outfile.write(browser.page_source)\n",
    "                    #Stop waiting\n",
    "                    break\n",
    "                if elapsed_time > 10: \n",
    "                    print(\"Timed out\")\n",
    "                    break\n",
    "        #Stop waiting\n",
    "        break\n",
    "\n",
    "print(\"Done\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With pages downloaded, scrape the data within each into a single dataframe\n",
    "* Read all html files in the folder\n",
    "* Open content as soup\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlFiles = glob.glob(\".\\\\Pages\\\\*.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(df,util,yr):\n",
    "    tbl = df.iloc[3:]\n",
    "    tbl.columns = df.iloc[2]\n",
    "    tbl.insert(0,'Src_Utility',util)\n",
    "    tbl.insert(1,'Src_Year',yr)\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410_WSC,2016\t439_WSC,2016\tABERNATHY,2016\tABILENE,2016\tABLES_SPRINGS_WSC,2016\tACTON_MUD,2016\tADDISON,2016\tAFTON_GROVE_WSC,2016\tAGUA_SUD,2016\tAIRLINE_MOBILE_HOME_PARK_LTD,2016\tAIR_FORCE_VILLAGE_II_INC,2016\tALAMO,2016\tALAMO_HEIGHTS,2016\tALBANY,2016\tALEDO,2016\tALGONQUIN_WATER_RESOURCES_OF_TEXAS,2016\tALICE,2016\tALLEN,2016\tALPINE,2016\tALTO,2016\tALTO_RURAL_WSC,2016\tALVARADO,2016\tALVIN,2016\tALVORD,2016\tAMARILLO,2016\tAMHERST,2016\tANAHUAC,2016\tANDERSON_COUNTY_CEDAR_CREEK_WSC,2016\tANDREWS,2016\tANGELINA_WSC,2016\tANGLETON,2016\tANNA,2016\tANNETTA,2016\tANSON,2016\tANTHONY,2016\tANTON,2016\tAPPLEBY_WSC,2016\tAQUA_WSC,2016\tARANSAS_PASS,2016\tARCHER_CITY,2016\tARCHER_COUNTY_MUD_1,2016\tARGYLE_WSC,2016\tARLEDGE_RIDGE_WSC,2016\tARLINGTON,2016\tARMSTRONG_WSC,2016\tARP,2016\tASHERTON,2016\tASPERMONT,2016\tATASCOSA_RURAL_WSC,2016\tATHENS,2016\tATLANTA,2016\t"
     ]
    }
   ],
   "source": [
    "for htmlFile in htmlFiles:\n",
    "    #Get utility and name from the file\n",
    "    utility = htmlFile.split(\"\\\\\")[-1][:-10]\n",
    "    year = htmlFile[-9:-5]\n",
    "    print(\"{},{}\".format(utility,year),end=\"\\t\")\n",
    "\n",
    "    #Read the contents\n",
    "    content = open(htmlFile,'r').read()\n",
    "    soup = BeautifulSoup(content,'lxml') \n",
    "\n",
    "    #Parse \n",
    "    bigDiv = soup.find(id='VisibleReportContentReportViewerControl_ctl09')\n",
    "    tbls = bigDiv.contents[0].contents[0].contents[0].contents[0].contents[0].contents[0].contents[0]\n",
    "\n",
    "    #Convert results to dataframes\n",
    "    dfs = pd.read_html(str(tbls))\n",
    "\n",
    "    #Pull data from tables\n",
    "    Table1 = getData(dfs[2],utility,year)\n",
    "    Table2 = getData(dfs[3],utility,year)\n",
    "    Table3 = getData(dfs[4],utility,year)\n",
    "    Table4 = getData(dfs[5],utility,year)\n",
    "    #Table5 = getData(dfs[6],utility,year)\n",
    "\n",
    "    #Merge data\n",
    "    if htmlFile == htmlFiles[0]:\n",
    "        df1 = Table1\n",
    "        df2 = Table2\n",
    "        df3 = Table3\n",
    "        df4 = Table4\n",
    "        #df5 = Table5\n",
    "    else:\n",
    "        df1 = pd.concat((df1,Table1))\n",
    "        df2 = pd.concat((df2,Table2))\n",
    "        df3 = pd.concat((df3,Table3))\n",
    "        df4 = pd.concat((df4,Table4))\n",
    "        #df5 = pd.concat((df5,Table5))\n",
    "    \n",
    "#Write tables out\n",
    "df1.to_csv(\"Table1.csv\",index=False)\n",
    "df2.to_csv(\"Table2.csv\",index=False)\n",
    "df3.to_csv(\"Table3.csv\",index=False)\n",
    "df4.to_csv(\"Table4.csv\",index=False)\n",
    "#df5.to_csv(\"Table5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"Table1.csv\",index=False)\n",
    "df2.to_csv(\"Table2.csv\",index=False)\n",
    "df3.to_csv(\"Table3.csv\",index=False)\n",
    "df4.to_csv(\"Table4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDict = {}\n",
    "for dfX in dfs:\n",
    "    if dfX.shape[1] < 2: continue\n",
    "    if dfX.iloc[1,0] == 'Table 1. Calculated GPCD of the WUG Utility': dfDict['1']=dfX\n",
    "    if dfX.iloc[1,0] == 'Table 2. Water System(s) of the WUG Utility': dfDict['2']=dfX\n",
    "    if dfX.iloc[1,0] == 'Table 3. Intake Volumes of Water Systems of the WUG Utility': dfDict['3']=dfX\n",
    "    if dfX.iloc[1,0] == 'Table 4. Sales Volumes of Water Systems of the WUG Utility': dfDict['4']=dfX  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[6].iloc[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
